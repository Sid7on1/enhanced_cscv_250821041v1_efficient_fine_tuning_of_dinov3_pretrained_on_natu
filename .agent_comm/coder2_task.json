{
  "agent_id": "coder2",
  "task_id": "task_5",
  "files": [
    {
      "name": "README.md",
      "purpose": "Project documentation",
      "priority": "medium"
    },
    {
      "name": "utils.py",
      "purpose": "Utility functions",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CV_2508.21041v1_Efficient_Fine_Tuning_of_DINOv3_Pretrained_on_Natu",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.CV_2508.21041v1_Efficient-Fine-Tuning-of-DINOv3-Pretrained-on-Natu with content analysis. Detected project type: computer vision (confidence score: 6 matches).",
    "key_algorithms": [
      "Submitted",
      "Object",
      "Research",
      "Their"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.CV_2508.21041v1_Efficient-Fine-Tuning-of-DINOv3-Pretrained-on-Natu.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nEfficient Fine-Tuning of DINOv3 Pretrained on\nNatural Images for Atypical Mitotic Figure\nClassification in MIDOG 2025\nGuillaume Balezo1, 2, Rapha\u00ebl Bourgade1, and Thomas Walter1\n1Center for Computational Biology, Mines Paris PSL, Paris, France\n2Sanofi, Paris, France\nAtypical mitotic figures (AMFs) are markers of abnormal cell\ndivision associated with poor prognosis, yet their detection re-\nmains difficult due to low prevalence, subtle morphology, and\ninter-observer variability. The MIDOG 2025 challenge in-\ntroduces a benchmark for AMF classification across multiple\ndomains. In this work, we evaluate the recently published\nDINOv3-H+ vision transformer, pretrained on natural images,\nwhich we fine-tuned using low-rank adaptation (LoRA, 650k\ntrainable parameters) and extensive augmentation. Despite the\ndomain gap, DINOv3 transfers effectively to histopathology,\nachieving a balanced accuracy of 0.8871 on the preliminary test\nset. These results highlight the robustness of DINOv3 pretrain-\ning and show that, when combined with parameter-efficient\nfine-tuning, it provides a strong baseline for atypical mitosis\nclassification in MIDOG 2025.\nhistopathology | foundation models | atypical mitotic figures\nCorrespondence: guillaume.balezo@minesparis.psl.eu\nIntroduction\nMitotic activity is a central indicator of tumor proliferation\nand prognosis. Beyond simple counts, the distinction be-\ntween normal mitotic figures (NMFs) and atypical mitotic\nfigures (AMFs) is of particular interest, as AMFs reflect ab-\nnormal cell division processes and correlate with poor clini-\ncal outcome. However, their identification is challenging due\nto low prevalence, subtle morphological differences, and low\ninter-rater agreement even among trained pathologists. Auto-\nmated image analysis methods therefore have the potential to\nimprove reproducibility and reduce observer bias in this task.\nThe Mitosis Domain Generalization Challenge 2025 (MI-\nDOG25) extends the scope of previous editions (MIDOG\n2021 (1) and MIDOG 2022 (2)) with the goal of advancing\nAI-assisted cancer diagnosis. The Task 2 introduces a dedi-\ncated benchmark for AMF classification, where participants\nare asked to classify cropped cell patches (128\u00d7128 pixels)\ninto NMF or AMF across multiple tumor types, species,\nscanners, and laboratories. The dataset comprises more than\n12,000 annotated mitotic figures, with AMFs accounting for\nonly\u223c20% of cases, and the evaluation metric of the chal-\nlenge is the balanced accuracy to mitigate the strong class\nimbalance. Similar to the earlier MIDOG challenges, this\nbenchmark addresses the crucial problem of robustness and\ngeneralization across domains, now extended to the clinically\nrelevant task of atypical mitosis classification.In this work, we tackle the Task 2 by applying low-rank\nadaptation (LoRA) (3) to fine-tune the recently published\nDINOv3-H+ vision transformer (ViT) (4), a model pre-\ntrained on natural images using DINOv3 self-supervised\n(SSL) method. To enhance robustness across diverse do-\nmains and compensate for the limited number of atypical fig-\nures, we combine this strategy with extensive data augmen-\ntation. Our approach aims to test and leverage the represen-\ntational power of this new self-supervised pretraining, while\nensuring efficient adaptation to the heterogeneous challenge\ndataset.\nMaterial and Methods\nA. Dataset. The MIDOG 2025 atypical mitosis training set\nis derived from 454 histopathology images spanning nine do-\nmains defined by different tumor types, species, scanners,\nand laboratories. Each mitotic figure was subtyped as normal\nor atypical by three expert pathologists in a blinded majority-\nvote setting.\nIn addition to the official MIDOG 2025 atypical training set,\nwe incorporated three external resources. The AMi-Br (5)\ndataset provides mitotic figures from MIDOG 2021 (1) and\nTUPAC16 (6); to avoid overlap, we only used the TUPAC16\ncohort. The AtNorM-Br (7) dataset contains mitotic figures\nfrom the TCGA (8) breast cancer cohort, annotated by an\nexpert pathologist. Finally, the OMG-Octo dataset (9) was\ncreated by screening large histopathology data with a model\npretrained on AMi-Br and MIDOG25, followed by expert re-\nview of candidate mitoses.\nAfter removing duplicate images, our training set comprised\n11,939 mitotic figures from MIDOG 2025 (10,191 normal,\n1,748 atypical), 1,999 mitotic figures from AMi-Br (1,571\nnormal, 428 atypical), 711 from AtNorM-Br (587 normal,\n124 atypical), and 1,752 from OMG-Octo (378 normal, 1,374\natypical), resulting in a total of 16,398 figures (12,724 normal\nand 3,674 atypical). All datasets were provided as 128\u00d7128\npixel crops centered on the mitotic figure, except OMG-Octo,\nwhich was originally 64\u00d764 pixels and resized to 128\u00d7128\nfor training, corresponding to a resolution of 0.25\u00b5m/pixel.\nThe preliminary test set provided for the Task 2 consisted of\nmitotic figure crops from four tumor types not included in\nthe final test data. It was made available on the challenge\nplatform two weeks prior to submission for debugging pur-\nposes. The final test set consisted of 120 cases covering 12\nGuillaume Balezo et al. | 1arXiv:2508.21041v1  [eess.IV]  28 Aug 2025\n\n--- Page 2 ---\ndistinct tumor types from both human and veterinary pathol-\nogy, with 10 cases per tumor type. This set spans multiple\nlaboratories and scanning systems and was used for the of-\nficial evaluation. Performance was assessed using balanced\naccuracy, computed over all patches of the test set.\nB. Network Training. We trained our model on 128 \u00d7128\npixel image crops, matching the challenge\u2019s original patch\nsize. Our model is a DINOv3-H+ vision transformer pre-\ntrained on the LVD-168M natural image dataset, which we\nfine-tuned for the Task 2 with low-rank adaptation (LoRA;\nrank = 4,\u03b1LoRA = 8.0, dropout 0.05, applied only to the\nquery and value projections in the attention layers), resulting\nin only about 650k trainable parameters. A linear classifica-\ntion head with 0.5 dropout was added to produce logits from\nthe class token. Training was run with a batch size of 16 and\nmixed precision (FP16). We optimized with AdamW (learn-\ning rate 1\u00d710\u22124, weight decay 0.1, \u03f5= 1\u00d710\u22127), using a\ncosine schedule with linear warmup during the first 10% of\ntraining (from 8.47\u00d710\u22127to the base rate). Gradient norms\nwere clipped at 1.0 for stability.\nTo address the class imbalance ( \u223c20% atypical), we trained\nwith Focal Loss (10) ( \u03b1= 0.25,\u03b3= 2). Extensive online aug-\nmentations were applied, including color jitter, JPEG com-\npression, stain augmentation (multi-Macenko (11, 12) with\nrandom stain domain references), defocus blur, affine trans-\nforms, D4 symmetry, coarse dropout (up to two random\nboxes), and a custom black-border augmentation to mimic\nzero-padded regions in the training data. Inputs were nor-\nmalized with ImageNet statistics, consistent with DINOv3\npretraining. The final submitted model was trained for 60\nepochs using all available datasets (AMi-Br TUPAC16, MI-\nDOG25, AtNorM-Br, and OMG-Octo). At inference, we em-\nployed test-time augmentation by averaging logits across four\nrotated views to improve robustness.\nThe multi-Macenko augmentation used 10 stain references\nper domain, extracted from the training set as well as MITOS\nCMC (13), MITOS CCMCT (14), and TCGA COAD/BLCA\ncases. During training, a domain and number of references\nwere sampled at random to mimic diverse staining condi-\ntions.\nEvaluation and Results\nWe evaluated our method with 4-fold cross-validation on the\ncomplete training data, holding out the AMi-Br TUPAC16\nsubset as an external test set. In each fold, the model was\nevaluated on the validation fold, as well as on the AMi-Br\nTUPAC16 test set and the preliminary test set. We report\nmean and standard deviation across folds for balanced accu-\nracy (BA).\nTable 1. Performance of our method on 4-fold cross-validation (mean \u00b1 std), the\nexternal AMi-Br (TUPAC) test set, and the preliminary test set.\nSplit BA\nCross-validation 0.927 \u00b10.002\nAMi-Br TUPAC test 0.842 \u00b10.004\nPreliminary Test Set 0.887We also explored continuing the self-supervised training of\nthe DINOv3-L (LVD) model on mitosis-like images obtained\nfrom an object detector trained on Task 1. Candidate patches\nwere collected from TCGA BLCA, COAD, MITOS CMC,\nMITOS CCMCT, and the challenge training data, resulting\nin a dataset of \u223c260k crops (128\u00d7128). Using the official\nDINOv3 pipeline, we fine-tuned LoRA parameters starting\nfrom ImageNet-pretrained weights. Due to limited compute\n(4 GPUs, batch size 4 vs. the original 2048), training was\nconstrained. Linear probing on the training set showed that\nthis additional pretraining yielded a relative improvement of\napproximately 10% in linear balanced accuracy, increasing\nfrom 53.44 % with ImageNet initialization to 63.11 %. This\nsuggests that large-scale SSL on mitosis images could be\nbeneficial. However, due to time and budget constraints, we\ndid not attempt full LoRA fine-tuning of this model, leaving\nit as a promising direction for future work.\nDiscussion\nIn this work, we showed that DINOv3-H+ with LoRA fine-\ntuning provides a strong baseline for atypical mitosis clas-\nsification in MIDOG 2025, while requiring training of only\n\u223c650k parameters. The robust pretraining of DINOv3 on\nnatural images appears to transfer well to histopathology,\neven though the domain shift is substantial. On the prelim-\ninary test set, our model achieved a balanced accuracy of\n0.8871. These results suggest that parameter-efficient adapta-\ntion combined with extensive augmentation can achieve com-\npetitive performance under limited data and severe class im-\nbalance.\nFuture work could extend this study with broader validation\nand exploration of alternative DINOv3 variants (e.g., Large,\n7B) and large-scale self-supervised pretraining directly on\nhistopathology data. Such efforts may further improve gen-\neralization across domains and strengthen the applicability of\nfoundation models for mitosis subtyping.\nBibliography\n1. Marc Aubreville, Nikolas Stathonikos, Christof A Bertram, Robert Klopleisch, Natalie ter\nHoeve, Francesco Ciompi, Frauke Wilm, Christian Marzahl, Taryn A Donovan, Andreas\nMaier, et al. Mitosis domain generalization in histopathology images\u2013the midog challenge.\narXiv preprint arXiv:2204.03742 , 2022.\n2. Marc Aubreville, Christof Bertram, Katharina Breininger, Samir Jabari, Nikolas Stathonikos,\nand Mitko Veta. Mitosis domain generalization challenge 2022. In 25th International Con-\nference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2022) ,\n2022. doi: 10.5281/zenodo.6362337 .\n3. Edward J Hu, Y elong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR ,\n1(2):3, 2022.\n4. Oriane Sim\u00e9oni, Huy V Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo\nJose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Micha\u00ebl Ramamonjisoa, et al. Dinov3.\narXiv preprint arXiv:2508.10104 , 2025.\n5. Christof A. Bertram, Viktoria Weiss, Taryn A. Donovan, Sweta Banerjee, Thomas Conrad,\nJonas Ammeling, Robert Klopfleisch, Christopher Kaltenecker, and Marc Aubreville. His-\ntologic dataset of normal and atypical mitotic figures on human breast cancer (ami-br). In\nChristoph Palm, Katharina Breininger, Thomas Deserno, Heinz Handels, Andreas Maier,\nKlaus H. Maier-Hein, and Thomas M. Tolxdorff, editors, Bildverarbeitung f\u00fcr die Medizin\n2025 , pages 113\u2013118, Wiesbaden, 2025. Springer Fachmedien Wiesbaden. ISBN 978-3-\n658-47422-5.\n6. Mitko Veta, Yujing J. Heng, Nikolas Stathonikos, Babak Ehteshami Bejnordi, Francisco\nBeca, Thomas Wollmann, Karl Rohr, Manan A. Shah, Dayong Wang, Mikael Rousson,\nMartin Hedlund, David Tellez, Francesco Ciompi, Erwan Zerhouni, David Lanyi, Matheus\nViana, Vassili Kovalev, Vitali Liauchuk, Hady Ahmady Phoulady, Talha Qaiser, Simon Gra-\nham, Nasir Rajpoot, Erik Sj\u00f6blom, Jesper Molin, Kyunghyun Paeng, Sangheum Hwang,\nSunggyun Park, Zhipeng Jia, Eric I-Chao Chang, Y an Xu, Andrew H. Beck, Paul J. van Di-\nest, and Josien P .W. Pluim. Predicting breast tumor proliferation from whole-slide images:\n2 | Guillaume Balezo et al. DINOv3 PEFT for MIDOG 2025 ANMF Classification\n\n--- Page 3 ---\nB Network Training\nThe tupac16 challenge. Medical Image Analysis , 54:111\u2013121, 2019. ISSN 1361-8415. doi:\nhttps://doi.org/10.1016/j.media.2019.02.012 .\n7. Sweta Banerjee, Viktoria Weiss, Taryn A Donovan, Rutger HJ Fick, Thomas Conrad, Jonas\nAmmeling, Nils Porsche, Robert Klopfleisch, Christopher Kaltenecker, Katharina Breininger,\net al. Benchmarking deep learning and vision foundation models for atypical vs. normal\nmitosis classification with cross-dataset evaluation. arXiv preprint arXiv:2506.21444 , 2025.\n8. JN Cancer Genome Atlas Research Network et al. The cancer genome atlas pan-cancer\nanalysis project. Nat. Genet , 45(10):1113\u20131120, 2013.\n9. Z. Shen, M. A. Hawkins, E. Baer, K. Br\u00e4utigam, and C.-A. Collins Fekete. OMG-Octo Atypi-\ncal: A refinement of the original OMG-Octo database to incorporate atypical mitoses, 2025.\n[Data set].\n10. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense\nobject detection. In Proceedings of the IEEE international conference on computer vision ,\npages 2980\u20132988, 2017.\n11. Desislav Ivanov, Carlo Alberto Barbano, and Marco Grangetto. Multi-target stain normaliza-\ntion for histology slides. In International Workshop on Medical Optical Imaging and Virtual\nMicroscopy Image Analysis , pages 36\u201344. Springer, 2024.\n12. Marc Macenko, Marc Niethammer, James S Marron, David Borland, John T Woosley, Xi-\naojun Guan, Charles Schmitt, and Nancy E Thomas. A method for normalizing histology\nslides for quantitative analysis. In 2009 IEEE international symposium on biomedical imag-\ning: from nano to macro , pages 1107\u20131110. IEEE, 2009.\n13. Marc Aubreville, Christof A Bertram, Taryn A Donovan, Christian Marzahl, Andreas Maier,\nand Robert Klopfleisch. A completely annotated whole slide image dataset of canine breast\ncancer to aid human breast cancer research. Scientific data , 7(1):417, 2020.\n14. Christof A Bertram, Marc Aubreville, Christian Marzahl, Andreas Maier, and Robert\nKlopfleisch. A large-scale dataset for mitotic figure assessment on whole slide images\nof canine cutaneous mast cell tumor. Scientific data , 6(1):274, 2019.\nDINOv3 PEFT for MIDOG 2025 ANMF Classification bioR\u03c7iv | 3",
  "project_dir": "artifacts/projects/enhanced_cs.CV_2508.21041v1_Efficient_Fine_Tuning_of_DINOv3_Pretrained_on_Natu",
  "communication_dir": "artifacts/projects/enhanced_cs.CV_2508.21041v1_Efficient_Fine_Tuning_of_DINOv3_Pretrained_on_Natu/.agent_comm",
  "assigned_at": "2025-08-31T20:51:35.026311",
  "status": "assigned"
}